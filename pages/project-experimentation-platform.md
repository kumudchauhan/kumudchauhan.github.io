---
layout: page
title: A/B Testing & Experimentation Framework
description: Building a culture of experimentation for product decisions
---

<div class="page-hero">
  <h1>A/B Testing & Experimentation Framework</h1>
  <p class="subtitle">Data-driven product decisions at scale, Fetch Rewards</p>
</div>

<section>
  <div class="section-container">
    <div class="about-content">

      <div class="case-study-meta" style="margin-bottom: 2rem;">
        <span>A/B Testing</span>
        <span>Statistical Analysis</span>
        <span>Python</span>
        <span>SQL</span>
        <span>Looker</span>
      </div>

      <div class="case-study-section">
        <h4>Business Problem</h4>
        <p>As Fetch scaled to millions of active users, the product team was shipping features based on intuition and stakeholder opinions rather than measured impact. There was no standardized experimentation process, no consistent way to define success metrics, size experiments, or determine whether a feature change actually moved the needle. This led to shipping changes that sometimes hurt engagement without anyone realizing it until weeks later.</p>
      </div>

      <div class="case-study-section">
        <h4>Approach</h4>
        <p><strong style="color: #0f172a;">Experimentation Design</strong>: established the end-to-end experimentation process: hypothesis definition, primary and guardrail metric selection, sample size and power calculations, randomization strategy, and pre-registration of analysis plans to avoid p-hacking.</p>
        <p><strong style="color: #0f172a;">Statistical Rigor</strong>: implemented proper statistical methodology including significance thresholds, minimum detectable effect sizing, sequential testing for early stopping decisions, and segment-level analysis to detect heterogeneous treatment effects across user cohorts.</p>
        <p><strong style="color: #0f172a;">Guardrail Metrics</strong>: defined a set of company-wide guardrail metrics (e.g., daily scan rate, session frequency, uninstall rate) that every experiment had to monitor, ensuring that optimizing one metric didn't silently degrade another.</p>
        <p><strong style="color: #0f172a;">Automated Reporting</strong>: built experiment dashboards that tracked real-time results, confidence intervals, and metric movements. Enabled product managers to self-serve experiment status without waiting for analyst review.</p>
      </div>

      <div class="case-study-section">
        <h4>Impact</h4>
        <p>Established a culture of experimentation across the product organization. Product decisions shifted from opinion-driven to evidence-based, with experiments becoming a standard gate before feature launches. Increased experiment velocity and reduced decision turnaround time, enabling the team to iterate faster while protecting core engagement metrics.</p>
      </div>

      <p style="margin-top: 2rem;"><a href="/pages/projects.html">&larr; Back to Projects</a></p>

    </div>
  </div>
</section>
